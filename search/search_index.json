{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"tokenwiser Bag of, not words, but tricks! Goal \u00b6 We noticed that a lot of benchmarks relied on heavy-weight tools while they did not check if something more lightweight would also work. Maybe if we just apply some simple tricks on our tokens we won't need massive language models. The goal of this package is to contribute tricks to keep your NLP pipelines simple. These tricks are made available for spaCy, scikit-learn and vowpal wabbit. If you're looking for a tool that can add pretrained language models to scikit-learn pipelines as a benchmark you'll want to explore another tool: whatlies . Features \u00b6 Scikit-Learn Tools \u00b6 The following submodules contain features that might be useful. .textprep : Contains string pre-processing tools for scikit-learn. .pipeline : Contains extra pipeline components for scikit-learn. .wabbit : Contains a scikit-learn component based on vowpal wabbit . SpaCy Tools \u00b6 .component : Contains spaCy compatible components that might be added as a pipeline step. .extension : Contains spaCy compatible extensions that might be added manually.","title":"Home"},{"location":"index.html#goal","text":"We noticed that a lot of benchmarks relied on heavy-weight tools while they did not check if something more lightweight would also work. Maybe if we just apply some simple tricks on our tokens we won't need massive language models. The goal of this package is to contribute tricks to keep your NLP pipelines simple. These tricks are made available for spaCy, scikit-learn and vowpal wabbit. If you're looking for a tool that can add pretrained language models to scikit-learn pipelines as a benchmark you'll want to explore another tool: whatlies .","title":"Goal"},{"location":"index.html#features","text":"","title":"Features"},{"location":"index.html#scikit-learn-tools","text":"The following submodules contain features that might be useful. .textprep : Contains string pre-processing tools for scikit-learn. .pipeline : Contains extra pipeline components for scikit-learn. .wabbit : Contains a scikit-learn component based on vowpal wabbit .","title":"Scikit-Learn Tools"},{"location":"index.html#spacy-tools","text":".component : Contains spaCy compatible components that might be added as a pipeline step. .extension : Contains spaCy compatible extensions that might be added manually.","title":"SpaCy Tools"},{"location":"faq.html","text":"Why can't I use normal Pipeline objects with the spaCy API? \u00b6 Scikit-Learn assumes that data is trained via .fit ( X , y ). predict ( X ) . This is great when you've got a dataset fully in memory but it's not so great when your dataset is too big to fit in one go. This is a main reason why spaCy has an .update () API for their trainable pipeline components. It's similar to .partial_fit ( X ) in scikit-learn. You wouldn't train on a single batch of data. Instead you would iteratively train on subsets of the dataset. A big downside of the Pipeline API is that it cannot use .partial_fit ( X ) . Even if all the components on the inside are compatible, it forces you to use .fit ( X ) . That is why this library offers a PartialPipeline . It only allows for components that have .partial_fit implemented and it's these pipelines that can also comply with spaCy's .update () API. Note that all scikit-learn components offered by this library are compatible with the PartialPipeline . This includes everything from the tokeniser.textprep submodule. Can I train spaCy with scikit-learn from Jupyter? \u00b6 It's not our favorite way of doing things, but nobody is stopping you. import spacy from spacy import registry from spacy.training import Example from spacy.language import Language from tokenwiser.pipeline import PartialPipeline from tokenwiser.model.sklearnmod import SklearnCat from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier @Language . factory ( \"custom-sklearn-cat\" ) def make_sklearn_cat ( nlp , name , sklearn_model , label , classes ): return SklearnCat ( nlp , name , sklearn_model , label , classes ) @registry . architectures ( \"sklearn_model_basic_sgd.v1\" ) def make_sklearn_cat_basic_sgd (): \"\"\"This creates a *partial* pipeline. We can't use a standard pipeline from scikit-learn.\"\"\" return PartialPipeline ([( \"hash\" , HashingVectorizer ()), ( \"lr\" , SGDClassifier ( loss = \"log\" ))]) nlp = spacy . load ( \"en_core_web_sm\" ) config = { \"sklearn_model\" : \"@sklearn_model_basic_sgd.v1\" , \"label\" : \"pos\" , \"classes\" : [ \"pos\" , \"neg\" ] } nlp . add_pipe ( \"custom-sklearn-cat\" , config = config ) texts = [ \"you are a nice person\" , \"this is a great movie\" , \"i do not like cofee\" , \"i hate tea\" ] labels = [ \"pos\" , \"pos\" , \"neg\" , \"neg\" ] # This is the training loop just for out categorizer model. with nlp . select_pipes ( enable = \"custom-sklearn-cat\" ): optimizer = nlp . resume_training () for loop in range ( 10 ): for t , lab in zip ( texts , labels ): doc = nlp . make_doc ( t ) example = Example . from_dict ( doc , { \"cats\" : { \"pos\" : lab }}) nlp . update ([ example ], sgd = optimizer ) nlp ( \"you are a nice person\" ) . cats # {'pos': 0.9979167909733176} nlp ( \"coffee i do not like\" ) . cats # {'neg': 0.990049724779963}","title":"FAQ"},{"location":"faq.html#why-cant-i-use-normal-pipeline-objects-with-the-spacy-api","text":"Scikit-Learn assumes that data is trained via .fit ( X , y ). predict ( X ) . This is great when you've got a dataset fully in memory but it's not so great when your dataset is too big to fit in one go. This is a main reason why spaCy has an .update () API for their trainable pipeline components. It's similar to .partial_fit ( X ) in scikit-learn. You wouldn't train on a single batch of data. Instead you would iteratively train on subsets of the dataset. A big downside of the Pipeline API is that it cannot use .partial_fit ( X ) . Even if all the components on the inside are compatible, it forces you to use .fit ( X ) . That is why this library offers a PartialPipeline . It only allows for components that have .partial_fit implemented and it's these pipelines that can also comply with spaCy's .update () API. Note that all scikit-learn components offered by this library are compatible with the PartialPipeline . This includes everything from the tokeniser.textprep submodule.","title":"Why can't I use normal Pipeline objects with the spaCy API?"},{"location":"faq.html#can-i-train-spacy-with-scikit-learn-from-jupyter","text":"It's not our favorite way of doing things, but nobody is stopping you. import spacy from spacy import registry from spacy.training import Example from spacy.language import Language from tokenwiser.pipeline import PartialPipeline from tokenwiser.model.sklearnmod import SklearnCat from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier @Language . factory ( \"custom-sklearn-cat\" ) def make_sklearn_cat ( nlp , name , sklearn_model , label , classes ): return SklearnCat ( nlp , name , sklearn_model , label , classes ) @registry . architectures ( \"sklearn_model_basic_sgd.v1\" ) def make_sklearn_cat_basic_sgd (): \"\"\"This creates a *partial* pipeline. We can't use a standard pipeline from scikit-learn.\"\"\" return PartialPipeline ([( \"hash\" , HashingVectorizer ()), ( \"lr\" , SGDClassifier ( loss = \"log\" ))]) nlp = spacy . load ( \"en_core_web_sm\" ) config = { \"sklearn_model\" : \"@sklearn_model_basic_sgd.v1\" , \"label\" : \"pos\" , \"classes\" : [ \"pos\" , \"neg\" ] } nlp . add_pipe ( \"custom-sklearn-cat\" , config = config ) texts = [ \"you are a nice person\" , \"this is a great movie\" , \"i do not like cofee\" , \"i hate tea\" ] labels = [ \"pos\" , \"pos\" , \"neg\" , \"neg\" ] # This is the training loop just for out categorizer model. with nlp . select_pipes ( enable = \"custom-sklearn-cat\" ): optimizer = nlp . resume_training () for loop in range ( 10 ): for t , lab in zip ( texts , labels ): doc = nlp . make_doc ( t ) example = Example . from_dict ( doc , { \"cats\" : { \"pos\" : lab }}) nlp . update ([ example ], sgd = optimizer ) nlp ( \"you are a nice person\" ) . cats # {'pos': 0.9979167909733176} nlp ( \"coffee i do not like\" ) . cats # {'neg': 0.990049724779963}","title":"Can I train spaCy with scikit-learn from Jupyter?"},{"location":"api/component.html","text":"component \u00b6 from tokenwiser.component import * In the component submodule you can find spaCy compatible components. attach_sklearn_categoriser ( nlp , pipe_name , estimator ) \u00b6 This function will attach a scikit-learn compatible estimator to the pipeline which will feed predictions to the .cats property. This is useful if you're interesting in added a pre-trained sklearn model to the pipeline. This is not useful if you're interested in training a new model via spaCy, check out the tokenwiser.model submodule for that. Usage: import spacy from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.component import attach_sklearn_categoriser X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = [ \"pos\" , \"pos\" , \"pos\" , \"neg\" , \"neg\" , \"neg\" ] # Note that we're training a pipeline here via a single-batch `.fit()` method pipe = make_pipeline ( CountVectorizer (), LogisticRegression ()) . fit ( X , y ) nlp = spacy . load ( \"en_core_web_sm\" ) # This is where we attach our pre-trained model as a pipeline step. attach_sklearn_categoriser ( nlp , pipe_name = \"silly_sentiment\" , estimator = pipe ) assert nlp . pipe_names [ - 1 ] == \"silly_sentiment\" assert nlp ( \"this post i really like\" ) . cats [ \"pos\" ] > 0.5 Source code in tokenwiser/component/_sklearn.py def attach_sklearn_categoriser ( nlp , pipe_name , estimator ): \"\"\" This function will attach a scikit-learn compatible estimator to the pipeline which will feed predictions to the `.cats` property. This is useful if you're interesting in added a pre-trained sklearn model to the pipeline. This is **not** useful if you're interested in training a new model via spaCy, check out the `tokenwiser.model` submodule for that. Usage: ```python import spacy from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.component import attach_sklearn_categoriser X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = [\"pos\", \"pos\", \"pos\", \"neg\", \"neg\", \"neg\"] # Note that we're training a pipeline here via a single-batch `.fit()` method pipe = make_pipeline(CountVectorizer(), LogisticRegression()).fit(X, y) nlp = spacy.load(\"en_core_web_sm\") # This is where we attach our pre-trained model as a pipeline step. attach_sklearn_categoriser(nlp, pipe_name=\"silly_sentiment\", estimator=pipe) assert nlp.pipe_names[-1] == \"silly_sentiment\" assert nlp(\"this post i really like\").cats[\"pos\"] > 0.5 ``` \"\"\" @Language . component ( pipe_name ) def my_component ( doc ): pred = estimator . predict ([ doc . text ])[ 0 ] proba = estimator . predict_proba ([ doc . text ]) . max () doc . cats [ pred ] = proba return doc nlp . add_pipe ( pipe_name )","title":"component"},{"location":"api/component.html#component","text":"from tokenwiser.component import * In the component submodule you can find spaCy compatible components.","title":"component"},{"location":"api/component.html#tokenwiser.component._sklearn.attach_sklearn_categoriser","text":"This function will attach a scikit-learn compatible estimator to the pipeline which will feed predictions to the .cats property. This is useful if you're interesting in added a pre-trained sklearn model to the pipeline. This is not useful if you're interested in training a new model via spaCy, check out the tokenwiser.model submodule for that. Usage: import spacy from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.component import attach_sklearn_categoriser X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = [ \"pos\" , \"pos\" , \"pos\" , \"neg\" , \"neg\" , \"neg\" ] # Note that we're training a pipeline here via a single-batch `.fit()` method pipe = make_pipeline ( CountVectorizer (), LogisticRegression ()) . fit ( X , y ) nlp = spacy . load ( \"en_core_web_sm\" ) # This is where we attach our pre-trained model as a pipeline step. attach_sklearn_categoriser ( nlp , pipe_name = \"silly_sentiment\" , estimator = pipe ) assert nlp . pipe_names [ - 1 ] == \"silly_sentiment\" assert nlp ( \"this post i really like\" ) . cats [ \"pos\" ] > 0.5 Source code in tokenwiser/component/_sklearn.py def attach_sklearn_categoriser ( nlp , pipe_name , estimator ): \"\"\" This function will attach a scikit-learn compatible estimator to the pipeline which will feed predictions to the `.cats` property. This is useful if you're interesting in added a pre-trained sklearn model to the pipeline. This is **not** useful if you're interested in training a new model via spaCy, check out the `tokenwiser.model` submodule for that. Usage: ```python import spacy from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.component import attach_sklearn_categoriser X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = [\"pos\", \"pos\", \"pos\", \"neg\", \"neg\", \"neg\"] # Note that we're training a pipeline here via a single-batch `.fit()` method pipe = make_pipeline(CountVectorizer(), LogisticRegression()).fit(X, y) nlp = spacy.load(\"en_core_web_sm\") # This is where we attach our pre-trained model as a pipeline step. attach_sklearn_categoriser(nlp, pipe_name=\"silly_sentiment\", estimator=pipe) assert nlp.pipe_names[-1] == \"silly_sentiment\" assert nlp(\"this post i really like\").cats[\"pos\"] > 0.5 ``` \"\"\" @Language . component ( pipe_name ) def my_component ( doc ): pred = estimator . predict ([ doc . text ])[ 0 ] proba = estimator . predict_proba ([ doc . text ]) . max () doc . cats [ pred ] = proba return doc nlp . add_pipe ( pipe_name )","title":"attach_sklearn_categoriser()"},{"location":"api/extension.html","text":"extension \u00b6 from tokenwiser.extension import * In the extension submodule you can find spaCy compatible extensions. attach_hyphen_extension () \u00b6 This function will attach an extension ._.hyphen to the Token s. import spacy from tokenwiser.extension import attach_hyphen_extension nlp = spacy . load ( \"en_core_web_sm\" ) # Attach the Hyphen extensions. attach_hyphen_extension () # Now you can query hyphens on the tokens. doc = nlp ( \"this is a dinosaurhead\" ) tok = doc [ - 1 ] assert tok . _ . hyphen == [ \"di\" , \"no\" , \"saur\" , \"head\" ] Source code in tokenwiser/extension/_extension.py def attach_hyphen_extension (): \"\"\" This function will attach an extension `._.hyphen` to the `Token`s. ```python import spacy from tokenwiser.extension import attach_hyphen_extension nlp = spacy.load(\"en_core_web_sm\") # Attach the Hyphen extensions. attach_hyphen_extension() # Now you can query hyphens on the tokens. doc = nlp(\"this is a dinosaurhead\") tok = doc[-1] assert tok._.hyphen == [\"di\", \"no\", \"saur\", \"head\"] ``` \"\"\" Token . set_extension ( \"hyphen\" , getter = lambda t : HyphenTextPrep () . encode_single ( t . text ) . split ( \" \" ), force = True , ) sklearn_method ( estimator ) \u00b6 A helper to turn a scikit-learn estimator into a spaCy extension. Just in case you really wanted to do it manually. import spacy from spacy.tokens import Doc from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.extension import sklearn_method X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = [ \"pos\" , \"pos\" , \"pos\" , \"neg\" , \"neg\" , \"neg\" ] # First we train a (silly) model. mod = make_pipeline ( CountVectorizer (), LogisticRegression ()) . fit ( X , y ) # This is where we attach the scikit-learn model to spaCy as a method extension. Doc . set_extension ( \"sillysent_method\" , method = sklearn_method ( mod )) # This is where we attach the scikit-learn model to spaCy as a property extension. Doc . set_extension ( \"sillysent_prop\" , getter = sklearn_method ( mod )) # Demo nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( \"thank you, really nice\" ) doc . _ . sillysent_method () # {\"neg\": 0.4446964938410244, \"pos: 0.5553035061589756} doc . _ . sillysent_prop # {\"neg: 0.4446964938410244, \"pos\": 0.5553035061589756} Source code in tokenwiser/extension/_extension.py def sklearn_method ( estimator ): \"\"\" A helper to turn a scikit-learn estimator into a spaCy extension. Just in case you *really* wanted to do it manually. ```python import spacy from spacy.tokens import Doc from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.extension import sklearn_method X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = [\"pos\", \"pos\", \"pos\", \"neg\", \"neg\", \"neg\"] # First we train a (silly) model. mod = make_pipeline(CountVectorizer(), LogisticRegression()).fit(X, y) # This is where we attach the scikit-learn model to spaCy as a method extension. Doc.set_extension(\"sillysent_method\", method=sklearn_method(mod)) # This is where we attach the scikit-learn model to spaCy as a property extension. Doc.set_extension(\"sillysent_prop\", getter=sklearn_method(mod)) # Demo nlp = spacy.load(\"en_core_web_sm\") doc = nlp(\"thank you, really nice\") doc._.sillysent_method() # {\"neg\": 0.4446964938410244, \"pos: 0.5553035061589756} doc._.sillysent_prop # {\"neg: 0.4446964938410244, \"pos\": 0.5553035061589756} ``` \"\"\" def method ( doc ): proba = estimator . predict_proba ([ doc . text ])[ 0 ] return { c : p for c , p in zip ( estimator . classes_ , proba )} return method","title":"extension"},{"location":"api/extension.html#extension","text":"from tokenwiser.extension import * In the extension submodule you can find spaCy compatible extensions.","title":"extension"},{"location":"api/extension.html#tokenwiser.extension._extension.attach_hyphen_extension","text":"This function will attach an extension ._.hyphen to the Token s. import spacy from tokenwiser.extension import attach_hyphen_extension nlp = spacy . load ( \"en_core_web_sm\" ) # Attach the Hyphen extensions. attach_hyphen_extension () # Now you can query hyphens on the tokens. doc = nlp ( \"this is a dinosaurhead\" ) tok = doc [ - 1 ] assert tok . _ . hyphen == [ \"di\" , \"no\" , \"saur\" , \"head\" ] Source code in tokenwiser/extension/_extension.py def attach_hyphen_extension (): \"\"\" This function will attach an extension `._.hyphen` to the `Token`s. ```python import spacy from tokenwiser.extension import attach_hyphen_extension nlp = spacy.load(\"en_core_web_sm\") # Attach the Hyphen extensions. attach_hyphen_extension() # Now you can query hyphens on the tokens. doc = nlp(\"this is a dinosaurhead\") tok = doc[-1] assert tok._.hyphen == [\"di\", \"no\", \"saur\", \"head\"] ``` \"\"\" Token . set_extension ( \"hyphen\" , getter = lambda t : HyphenTextPrep () . encode_single ( t . text ) . split ( \" \" ), force = True , )","title":"attach_hyphen_extension()"},{"location":"api/extension.html#tokenwiser.extension._extension.sklearn_method","text":"A helper to turn a scikit-learn estimator into a spaCy extension. Just in case you really wanted to do it manually. import spacy from spacy.tokens import Doc from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.extension import sklearn_method X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = [ \"pos\" , \"pos\" , \"pos\" , \"neg\" , \"neg\" , \"neg\" ] # First we train a (silly) model. mod = make_pipeline ( CountVectorizer (), LogisticRegression ()) . fit ( X , y ) # This is where we attach the scikit-learn model to spaCy as a method extension. Doc . set_extension ( \"sillysent_method\" , method = sklearn_method ( mod )) # This is where we attach the scikit-learn model to spaCy as a property extension. Doc . set_extension ( \"sillysent_prop\" , getter = sklearn_method ( mod )) # Demo nlp = spacy . load ( \"en_core_web_sm\" ) doc = nlp ( \"thank you, really nice\" ) doc . _ . sillysent_method () # {\"neg\": 0.4446964938410244, \"pos: 0.5553035061589756} doc . _ . sillysent_prop # {\"neg: 0.4446964938410244, \"pos\": 0.5553035061589756} Source code in tokenwiser/extension/_extension.py def sklearn_method ( estimator ): \"\"\" A helper to turn a scikit-learn estimator into a spaCy extension. Just in case you *really* wanted to do it manually. ```python import spacy from spacy.tokens import Doc from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import LogisticRegression from tokenwiser.extension import sklearn_method X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = [\"pos\", \"pos\", \"pos\", \"neg\", \"neg\", \"neg\"] # First we train a (silly) model. mod = make_pipeline(CountVectorizer(), LogisticRegression()).fit(X, y) # This is where we attach the scikit-learn model to spaCy as a method extension. Doc.set_extension(\"sillysent_method\", method=sklearn_method(mod)) # This is where we attach the scikit-learn model to spaCy as a property extension. Doc.set_extension(\"sillysent_prop\", getter=sklearn_method(mod)) # Demo nlp = spacy.load(\"en_core_web_sm\") doc = nlp(\"thank you, really nice\") doc._.sillysent_method() # {\"neg\": 0.4446964938410244, \"pos: 0.5553035061589756} doc._.sillysent_prop # {\"neg: 0.4446964938410244, \"pos\": 0.5553035061589756} ``` \"\"\" def method ( doc ): proba = estimator . predict_proba ([ doc . text ])[ 0 ] return { c : p for c , p in zip ( estimator . classes_ , proba )} return method","title":"sklearn_method()"},{"location":"api/model.html","text":"model \u00b6 from tokenwiser.model import * In the model submodule you can find scikit-learn pipelines that are trainable via spaCy. These pipelines apply the .partial_fit (). predict () -design which makes them compliant with the spacy train command. SklearnCat \u00b6 This is a spaCy pipeline component object that can train specific scikit-learn pipelines. This allows you to run a simple benchmark via spaCy on simple text-based scikit-learn models. One should not expect these models to have state of the art accuracy. But they should have \"pretty good\" accuracy while being substantially faster to train than most deep-learning based models. The intended use-case for these models is to offer a base benchmark. If these models perform well one your task, it's an indication that you're in luck and that you've got a simple task that doesn't require state of the art models.","title":"`model`"},{"location":"api/model.html#model","text":"from tokenwiser.model import * In the model submodule you can find scikit-learn pipelines that are trainable via spaCy. These pipelines apply the .partial_fit (). predict () -design which makes them compliant with the spacy train command.","title":"model"},{"location":"api/model.html#tokenwiser.model.sklearnmod.SklearnCat","text":"This is a spaCy pipeline component object that can train specific scikit-learn pipelines. This allows you to run a simple benchmark via spaCy on simple text-based scikit-learn models. One should not expect these models to have state of the art accuracy. But they should have \"pretty good\" accuracy while being substantially faster to train than most deep-learning based models. The intended use-case for these models is to offer a base benchmark. If these models perform well one your task, it's an indication that you're in luck and that you've got a simple task that doesn't require state of the art models.","title":"SklearnCat"},{"location":"api/pipeline.html","text":"pipeline \u00b6 from tokenwiser.pipeline import * In the pipeline submodule you can find scikit-learn compatbile pipelines that extend the standard behavior. PartialPipeline ( Pipeline ) \u00b6 Utility function to generate a PartialPipeline Parameters: Name Type Description Default steps a collection of text-transformers required from tokenwiser.pipeline import PartialPipeline from tokenwiser.textprep import HyphenTextPrep , Cleaner tc = PartialPipeline ([( 'clean' , Cleaner ()), ( 'hyp' , HyphenTextPrep ())]) data = [ \"dinosaurhead\" , \"another$$ sentence$$\" ] results = tc . partial_fit ( data ) . transform ( data ) expected = [ 'di no saur head' , 'an other sen tence' ] assert results == expected partial_fit ( self , X , y = None , classes = None , ** kwargs ) \u00b6 Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_pipe.py def partial_fit ( self , X , y = None , classes = None , ** kwargs ): \"\"\" Fits the components, but allow for batches. \"\"\" for name , step in self . steps : if not hasattr ( step , \"partial_fit\" ): raise ValueError ( f \"Step { name } is a { step } which does not have `.partial_fit` implemented.\" ) for name , step in self . steps : if hasattr ( step , \"predict\" ): step . partial_fit ( X , y , classes = classes , ** kwargs ) else : step . partial_fit ( X , y ) if hasattr ( step , \"transform\" ): X = step . transform ( X ) return self TextConcat ( BaseEstimator ) \u00b6 A component like FeatureUnion but this also concatenates the text. Parameters: Name Type Description Default transformer_list list of (name, text-transformer)-tuples required Examples: from tokenwiser.textprep import HyphenTextPrep , Cleaner from tokenwiser.pipeline import TextConcat tc = TextConcat ([( \"hyp\" , HyphenTextPrep ()), ( \"clean\" , Cleaner ())]) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected partial_fit ( self , X , y = None ) \u00b6 Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_concat.py def partial_fit ( self , X , y = None ): \"\"\" Fits the components, but allow for batches. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( \"Make sure that the names of each step are unique.\" ) return self PartialFeatureUnion ( FeatureUnion ) \u00b6 A PartialFeatureUnion is a FeatureUnion but able to .partial_fit . Parameters: Name Type Description Default transformer_list a list of transformers to apply and concatenate required Examples: import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep from tokenwiser.pipeline import PartialPipeline , PartialFeatureUnion pipe = PartialPipeline ([ ( \"clean\" , Cleaner ()), ( \"union\" , PartialFeatureUnion ([ ( \"full_text_pipe\" , PartialPipeline ([ ( \"identity\" , Identity ()), ( \"hash1\" , HashingVectorizer ()), ])), ( \"hyphen_pipe\" , PartialPipeline ([ ( \"hyphen\" , HyphenTextPrep ()), ( \"hash2\" , HashingVectorizer ()), ])) ])), ( \"clf\" , SGDClassifier ()) ]) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) for loop in range ( 3 ): pipe . partial_fit ( X , y , classes = [ 0 , 1 ]) assert np . all ( pipe . predict ( X ) == np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ])) partial_fit ( self , X , y = None , classes = None , ** kwargs ) \u00b6 Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_union.py def partial_fit ( self , X , y = None , classes = None , ** kwargs ): \"\"\" Fits the components, but allow for batches. \"\"\" for name , step in self . transformer_list : if not hasattr ( step , \"partial_fit\" ): raise ValueError ( f \"Step { name } is a { step } which does not have `.partial_fit` implemented.\" ) for name , step in self . transformer_list : if hasattr ( step , \"predict\" ): step . partial_fit ( X , y , classes = classes , ** kwargs ) else : step . partial_fit ( X , y ) return self make_partial_pipeline ( * steps ) \u00b6 Utility function to generate a PartialPipeline Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.pipeline import make_partial_pipeline from tokenwiser.textprep import HyphenTextPrep , Cleaner tc = make_partial_pipeline ( Cleaner (), HyphenTextPrep ()) data = [ \"dinosaurhead\" , \"another$$ sentence$$\" ] results = tc . partial_fit ( data ) . transform ( data ) expected = [ 'di no saur head' , 'an other sen tence' ] assert results == expected Source code in tokenwiser/pipeline/_pipe.py def make_partial_pipeline ( * steps ): \"\"\" Utility function to generate a `PartialPipeline` Arguments: steps: a collection of text-transformers ```python from tokenwiser.pipeline import make_partial_pipeline from tokenwiser.textprep import HyphenTextPrep, Cleaner tc = make_partial_pipeline(Cleaner(), HyphenTextPrep()) data = [\"dinosaurhead\", \"another$$ sentence$$\"] results = tc.partial_fit(data).transform(data) expected = ['di no saur head', 'an other sen tence'] assert results == expected ``` \"\"\" return PartialPipeline ( _name_estimators ( steps )) make_concat ( * steps ) \u00b6 Utility function to generate a TextConcat Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.textprep import HyphenTextPrep , Cleaner from tokenwiser.pipeline import make_concat tc = make_concat ( HyphenTextPrep (), Cleaner ()) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected Source code in tokenwiser/pipeline/_concat.py def make_concat ( * steps ): \"\"\" Utility function to generate a `TextConcat` Arguments: steps: a collection of text-transformers ```python from tokenwiser.textprep import HyphenTextPrep, Cleaner from tokenwiser.pipeline import make_concat tc = make_concat(HyphenTextPrep(), Cleaner()) results = tc.fit_transform([\"dinosaurhead\", \"another$$ sentence$$\"]) expected = ['di no saur head dinosaurhead', 'an other $$ sen tence$$ another sentence'] assert results == expected ``` \"\"\" return TextConcat ( _name_estimators ( steps )) make_partial_union ( * transformer_list ) \u00b6 Utility function to generate a PartialFeatureUnion Parameters: Name Type Description Default transformer_list a list of transformers to apply and concatenate () Examples: import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep from tokenwiser.pipeline import make_partial_pipeline , make_partial_union pipe = make_partial_pipeline ( Cleaner (), make_partial_union ( make_partial_pipeline ( Identity (), HashingVectorizer ()), make_partial_pipeline ( HyphenTextPrep (), HashingVectorizer ()) ), SGDClassifier () ) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) for loop in range ( 3 ): pipe . partial_fit ( X , y , classes = [ 0 , 1 ]) assert np . all ( pipe . predict ( X ) == np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ])) Source code in tokenwiser/pipeline/_union.py def make_partial_union ( * transformer_list ): \"\"\" Utility function to generate a `PartialFeatureUnion` Arguments: transformer_list: a list of transformers to apply and concatenate Example: ```python import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer from tokenwiser.textprep import Cleaner, Identity, HyphenTextPrep from tokenwiser.pipeline import make_partial_pipeline, make_partial_union pipe = make_partial_pipeline( Cleaner(), make_partial_union( make_partial_pipeline(Identity(), HashingVectorizer()), make_partial_pipeline(HyphenTextPrep(), HashingVectorizer()) ), SGDClassifier() ) X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = np.array([1, 1, 1, 0, 0, 0]) for loop in range(3): pipe.partial_fit(X, y, classes=[0, 1]) assert np.all(pipe.predict(X) == np.array([1, 1, 1, 0, 0, 0])) ``` \"\"\" return PartialFeatureUnion ( _name_estimators ( transformer_list ))","title":"pipeline"},{"location":"api/pipeline.html#pipeline","text":"from tokenwiser.pipeline import * In the pipeline submodule you can find scikit-learn compatbile pipelines that extend the standard behavior.","title":"pipeline"},{"location":"api/pipeline.html#tokenwiser.pipeline._pipe.PartialPipeline","text":"Utility function to generate a PartialPipeline Parameters: Name Type Description Default steps a collection of text-transformers required from tokenwiser.pipeline import PartialPipeline from tokenwiser.textprep import HyphenTextPrep , Cleaner tc = PartialPipeline ([( 'clean' , Cleaner ()), ( 'hyp' , HyphenTextPrep ())]) data = [ \"dinosaurhead\" , \"another$$ sentence$$\" ] results = tc . partial_fit ( data ) . transform ( data ) expected = [ 'di no saur head' , 'an other sen tence' ] assert results == expected","title":"PartialPipeline"},{"location":"api/pipeline.html#tokenwiser.pipeline._pipe.PartialPipeline.partial_fit","text":"Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_pipe.py def partial_fit ( self , X , y = None , classes = None , ** kwargs ): \"\"\" Fits the components, but allow for batches. \"\"\" for name , step in self . steps : if not hasattr ( step , \"partial_fit\" ): raise ValueError ( f \"Step { name } is a { step } which does not have `.partial_fit` implemented.\" ) for name , step in self . steps : if hasattr ( step , \"predict\" ): step . partial_fit ( X , y , classes = classes , ** kwargs ) else : step . partial_fit ( X , y ) if hasattr ( step , \"transform\" ): X = step . transform ( X ) return self","title":"partial_fit()"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.TextConcat","text":"A component like FeatureUnion but this also concatenates the text. Parameters: Name Type Description Default transformer_list list of (name, text-transformer)-tuples required Examples: from tokenwiser.textprep import HyphenTextPrep , Cleaner from tokenwiser.pipeline import TextConcat tc = TextConcat ([( \"hyp\" , HyphenTextPrep ()), ( \"clean\" , Cleaner ())]) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected","title":"TextConcat"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.TextConcat.partial_fit","text":"Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_concat.py def partial_fit ( self , X , y = None ): \"\"\" Fits the components, but allow for batches. \"\"\" names = [ n for n , t in self . transformer_list ] if len ( names ) != len ( set ( names )): raise ValueError ( \"Make sure that the names of each step are unique.\" ) return self","title":"partial_fit()"},{"location":"api/pipeline.html#tokenwiser.pipeline._union.PartialFeatureUnion","text":"A PartialFeatureUnion is a FeatureUnion but able to .partial_fit . Parameters: Name Type Description Default transformer_list a list of transformers to apply and concatenate required Examples: import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep from tokenwiser.pipeline import PartialPipeline , PartialFeatureUnion pipe = PartialPipeline ([ ( \"clean\" , Cleaner ()), ( \"union\" , PartialFeatureUnion ([ ( \"full_text_pipe\" , PartialPipeline ([ ( \"identity\" , Identity ()), ( \"hash1\" , HashingVectorizer ()), ])), ( \"hyphen_pipe\" , PartialPipeline ([ ( \"hyphen\" , HyphenTextPrep ()), ( \"hash2\" , HashingVectorizer ()), ])) ])), ( \"clf\" , SGDClassifier ()) ]) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) for loop in range ( 3 ): pipe . partial_fit ( X , y , classes = [ 0 , 1 ]) assert np . all ( pipe . predict ( X ) == np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]))","title":"PartialFeatureUnion"},{"location":"api/pipeline.html#tokenwiser.pipeline._union.PartialFeatureUnion.partial_fit","text":"Fits the components, but allow for batches. Source code in tokenwiser/pipeline/_union.py def partial_fit ( self , X , y = None , classes = None , ** kwargs ): \"\"\" Fits the components, but allow for batches. \"\"\" for name , step in self . transformer_list : if not hasattr ( step , \"partial_fit\" ): raise ValueError ( f \"Step { name } is a { step } which does not have `.partial_fit` implemented.\" ) for name , step in self . transformer_list : if hasattr ( step , \"predict\" ): step . partial_fit ( X , y , classes = classes , ** kwargs ) else : step . partial_fit ( X , y ) return self","title":"partial_fit()"},{"location":"api/pipeline.html#tokenwiser.pipeline._pipe.make_partial_pipeline","text":"Utility function to generate a PartialPipeline Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.pipeline import make_partial_pipeline from tokenwiser.textprep import HyphenTextPrep , Cleaner tc = make_partial_pipeline ( Cleaner (), HyphenTextPrep ()) data = [ \"dinosaurhead\" , \"another$$ sentence$$\" ] results = tc . partial_fit ( data ) . transform ( data ) expected = [ 'di no saur head' , 'an other sen tence' ] assert results == expected Source code in tokenwiser/pipeline/_pipe.py def make_partial_pipeline ( * steps ): \"\"\" Utility function to generate a `PartialPipeline` Arguments: steps: a collection of text-transformers ```python from tokenwiser.pipeline import make_partial_pipeline from tokenwiser.textprep import HyphenTextPrep, Cleaner tc = make_partial_pipeline(Cleaner(), HyphenTextPrep()) data = [\"dinosaurhead\", \"another$$ sentence$$\"] results = tc.partial_fit(data).transform(data) expected = ['di no saur head', 'an other sen tence'] assert results == expected ``` \"\"\" return PartialPipeline ( _name_estimators ( steps ))","title":"make_partial_pipeline()"},{"location":"api/pipeline.html#tokenwiser.pipeline._concat.make_concat","text":"Utility function to generate a TextConcat Parameters: Name Type Description Default steps a collection of text-transformers () from tokenwiser.textprep import HyphenTextPrep , Cleaner from tokenwiser.pipeline import make_concat tc = make_concat ( HyphenTextPrep (), Cleaner ()) results = tc . fit_transform ([ \"dinosaurhead\" , \"another$$ sentence$$\" ]) expected = [ 'di no saur head dinosaurhead' , 'an other $$ sen tence$$ another sentence' ] assert results == expected Source code in tokenwiser/pipeline/_concat.py def make_concat ( * steps ): \"\"\" Utility function to generate a `TextConcat` Arguments: steps: a collection of text-transformers ```python from tokenwiser.textprep import HyphenTextPrep, Cleaner from tokenwiser.pipeline import make_concat tc = make_concat(HyphenTextPrep(), Cleaner()) results = tc.fit_transform([\"dinosaurhead\", \"another$$ sentence$$\"]) expected = ['di no saur head dinosaurhead', 'an other $$ sen tence$$ another sentence'] assert results == expected ``` \"\"\" return TextConcat ( _name_estimators ( steps ))","title":"make_concat()"},{"location":"api/pipeline.html#tokenwiser.pipeline._union.make_partial_union","text":"Utility function to generate a PartialFeatureUnion Parameters: Name Type Description Default transformer_list a list of transformers to apply and concatenate () Examples: import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep from tokenwiser.pipeline import make_partial_pipeline , make_partial_union pipe = make_partial_pipeline ( Cleaner (), make_partial_union ( make_partial_pipeline ( Identity (), HashingVectorizer ()), make_partial_pipeline ( HyphenTextPrep (), HashingVectorizer ()) ), SGDClassifier () ) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) for loop in range ( 3 ): pipe . partial_fit ( X , y , classes = [ 0 , 1 ]) assert np . all ( pipe . predict ( X ) == np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ])) Source code in tokenwiser/pipeline/_union.py def make_partial_union ( * transformer_list ): \"\"\" Utility function to generate a `PartialFeatureUnion` Arguments: transformer_list: a list of transformers to apply and concatenate Example: ```python import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer from tokenwiser.textprep import Cleaner, Identity, HyphenTextPrep from tokenwiser.pipeline import make_partial_pipeline, make_partial_union pipe = make_partial_pipeline( Cleaner(), make_partial_union( make_partial_pipeline(Identity(), HashingVectorizer()), make_partial_pipeline(HyphenTextPrep(), HashingVectorizer()) ), SGDClassifier() ) X = [ \"i really like this post\", \"thanks for that comment\", \"i enjoy this friendly forum\", \"this is a bad post\", \"i dislike this article\", \"this is not well written\" ] y = np.array([1, 1, 1, 0, 0, 0]) for loop in range(3): pipe.partial_fit(X, y, classes=[0, 1]) assert np.all(pipe.predict(X) == np.array([1, 1, 1, 0, 0, 0])) ``` \"\"\" return PartialFeatureUnion ( _name_estimators ( transformer_list ))","title":"make_partial_union()"},{"location":"api/textprep.html","text":"textprep \u00b6 from tokenwiser.textprep import * In the textprep submodule you can find scikit-learn compatbile components that transform text into another type of text. The idea is that this may be combined in interesting ways in CountVectorizers. Cleaner ( TextPrep , BaseEstimator ) \u00b6 Applies a lowercase and removes non-alphanum. Usage: from tokenwiser.textprep import Cleaner single = Cleaner () . encode_single ( \"$$$5 dollars\" ) assert single == \"5 dollars\" multi = Cleaner () . transform ([ \"$$$5 dollars\" , \"#hashtag!\" ]) assert multi == [ \"5 dollars\" , \"hashtag\" ] Identity ( TextPrep , BaseEstimator ) \u00b6 Keeps the text as is. Can be used as a placeholder in a pipeline. Usage: from tokenwiser.textprep import Identity text = [ \"hello\" , \"world\" ] example = Identity () . transform ( text ) assert example == [ \"hello\" , \"world\" ] The main use-case is as a placeholder. from tokenwiser.pipeline import make_concat from sklearn.pipeline import make_pipeline , make_union from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), ) HyphenTextPrep ( TextPrep , BaseEstimator ) \u00b6 Hyphenate the text going in. Usage: from tokenwiser.textprep import HyphenTextPrep multi = HyphenTextPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ] SentencePiecePrep ( TextPrep , BaseEstimator ) \u00b6 The SentencePiecePrep object splits text into subtokens based on a pre-trained model. You can find many pre-trained subtokenizers via the bpemb project. For example, on the English sub-site you can find many models for different vocabulary sizes. Note that this site supports 275 pre-trained subword tokenizers. Note that you can train your own sentencepiece tokenizer as well. import sentencepiece as spm # This saves a file named `mod.model` which can be read in later. spm . SentencePieceTrainer . train ( '--input=tests/data/nlp.txt --model_prefix=mod --vocab_size=2000' ) Parameters: Name Type Description Default model_file pre-trained model file required Usage: from tokenwiser.textprep import SentencePiecePrep sp_tfm = SentencePiecePrep ( model_file = \"tests/data/en.vs5000.model\" ) texts = [ \"talking about geology\" ] example = sp_tfm . transform ( texts ) assert example == [ '\u2581talk ing \u2581about \u2581ge ology' ] download ( lang , vocab_size , filename = None ) classmethod \u00b6 Download a pre-trained model from the bpemb project. You can see some examples of pre-trained models on the English sub-site. There are many languages available, but you should take care that you pick the right vocabulary size. Parameters: Name Type Description Default lang str language code required vocab_size int vocab size, can be 1000, 3000, 5000, 10000, 25000, 50000, 100000, 200000 required Source code in tokenwiser/textprep/_sentpiece.py @classmethod def download ( self , lang : str , vocab_size : int , filename : str = None ): \"\"\" Download a pre-trained model from the bpemb project. You can see some examples of pre-trained models on the [English](https://nlp.h-its.org/bpemb/en/) sub-site. There are many languages available, but you should take care that you pick the right vocabulary size. Arguments: lang: language code vocab_size: vocab size, can be 1000, 3000, 5000, 10000, 25000, 50000, 100000, 200000 \"\"\" url = f \"https://bpemb.h-its.org/ { lang } / { lang } .wiki.bpe.vs { vocab_size } .model\" if not filename : filename = f \" { lang } .wiki.bpe.vs { vocab_size } .model\" try : urllib . request . urlretrieve ( url = url , filename = filename ) except HTTPError : raise ValueError ( f \"Double check if the language ( { lang } ) and voacb size ( { vocab_size } ) combo exist.\" ) PhoneticTextPrep ( TextPrep , BaseEstimator ) \u00b6 The ProneticPrep object prepares strings by encoding them phonetically. Parameters: Name Type Description Default kind type of encoding, either \"soundex\" , \" metaphone \" or \"nysiis\" required Usage: import spacy from tokenwiser.textprep import PhoneticTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = PhoneticTextPrep ( kind = \"soundex\" ) . transform ([ \"dinosaurus book\" ]) example2 = PhoneticTextPrep ( kind = \"metaphone\" ) . transform ([ \"dinosaurus book\" ]) example3 = PhoneticTextPrep ( kind = \"nysiis\" ) . transform ([ \"dinosaurus book\" ]) assert example1 [ 0 ] == 'D526 B200' assert example2 [ 0 ] == 'TNSRS BK' assert example3 [ 0 ] == 'DANASAR BAC' YakeTextPrep ( TextPrep , BaseEstimator ) \u00b6 Remove all text except meaningful key-phrases. Uses yake . Parameters: Name Type Description Default top_n number of key-phrases to select required unique only return unique keywords from the key-phrases required Usage: from tokenwiser.textprep import YakeTextPrep text = [ \"Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning\" ] example = YakeTextPrep ( top_n = 3 , unique = False ) . transform ( text ) assert example [ 0 ] == 'hosts data science acquiring kaggle google is acquiring' SpacyMorphTextPrep ( TextPrep , BaseEstimator ) \u00b6 Adds morphologic information to tokens in text. Usage: import spacy from tokenwiser.textprep import SpacyMorphTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyMorphTextPrep ( nlp ) . encode_single ( \"quick! duck!\" ) example2 = SpacyMorphTextPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"quick|Degree=Pos !|PunctType=Peri duck|Number=Sing !|PunctType=Peri\" assert example2 == \"hey| look|VerbForm=Inf a|Definite=Ind|PronType=Art duck|Number=Sing\" SpacyPosTextPrep ( TextPrep , BaseEstimator ) \u00b6 Adds part of speech information per token using spaCy. Parameters: Name Type Description Default model the spaCy model to use required lemma also lemmatize the text required fine_grained use fine grained parts of speech required Usage: import spacy from tokenwiser.textprep import SpacyPosTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyPosTextPrep ( nlp ) . encode_single ( \"we need to duck\" ) example2 = SpacyPosTextPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"we|PRON need|VERB to|PART duck|VERB\" assert example2 == \"hey|INTJ look|VERB a|DET duck|NOUN\" SpacyLemmaTextPrep ( TextPrep , BaseEstimator ) \u00b6 Turns each token into a lemmatizer version using spaCy. Usage: import spacy from tokenwiser.textprep import SpacyLemmaTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyLemmaTextPrep ( nlp ) . encode_single ( \"we are running\" ) example2 = SpacyLemmaTextPrep ( nlp ) . encode_single ( \"these are dogs\" ) assert example1 == 'we be run' assert example2 == 'these be dog' SnowballTextPrep ( TextPrep , BaseEstimator ) \u00b6 Applies the snowball stemmer to the text. There are 26 languages supported, for the full list check the list on the lefthand side on pypi . Usage: from tokenwiser.textprep import SnowballTextPrep single = SnowballTextPrep ( language = 'english' ) . encode_single ( \"Dogs like running\" ) assert single == \"Dog like run\" multi = Cleaner () . transform ([ \"Dogs like running\" , \"Cats like sleeping\" ]) assert multi == [ \"Dog like run\" , \"Cat like sleep\" ]","title":"textprep"},{"location":"api/textprep.html#textprep","text":"from tokenwiser.textprep import * In the textprep submodule you can find scikit-learn compatbile components that transform text into another type of text. The idea is that this may be combined in interesting ways in CountVectorizers.","title":"textprep"},{"location":"api/textprep.html#tokenwiser.textprep._cleaner.Cleaner","text":"Applies a lowercase and removes non-alphanum. Usage: from tokenwiser.textprep import Cleaner single = Cleaner () . encode_single ( \"$$$5 dollars\" ) assert single == \"5 dollars\" multi = Cleaner () . transform ([ \"$$$5 dollars\" , \"#hashtag!\" ]) assert multi == [ \"5 dollars\" , \"hashtag\" ]","title":"Cleaner"},{"location":"api/textprep.html#tokenwiser.textprep._identity.Identity","text":"Keeps the text as is. Can be used as a placeholder in a pipeline. Usage: from tokenwiser.textprep import Identity text = [ \"hello\" , \"world\" ] example = Identity () . transform ( text ) assert example == [ \"hello\" , \"world\" ] The main use-case is as a placeholder. from tokenwiser.pipeline import make_concat from sklearn.pipeline import make_pipeline , make_union from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), )","title":"Identity"},{"location":"api/textprep.html#tokenwiser.textprep._hyphen.HyphenTextPrep","text":"Hyphenate the text going in. Usage: from tokenwiser.textprep import HyphenTextPrep multi = HyphenTextPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ]","title":"HyphenTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._sentpiece.SentencePiecePrep","text":"The SentencePiecePrep object splits text into subtokens based on a pre-trained model. You can find many pre-trained subtokenizers via the bpemb project. For example, on the English sub-site you can find many models for different vocabulary sizes. Note that this site supports 275 pre-trained subword tokenizers. Note that you can train your own sentencepiece tokenizer as well. import sentencepiece as spm # This saves a file named `mod.model` which can be read in later. spm . SentencePieceTrainer . train ( '--input=tests/data/nlp.txt --model_prefix=mod --vocab_size=2000' ) Parameters: Name Type Description Default model_file pre-trained model file required Usage: from tokenwiser.textprep import SentencePiecePrep sp_tfm = SentencePiecePrep ( model_file = \"tests/data/en.vs5000.model\" ) texts = [ \"talking about geology\" ] example = sp_tfm . transform ( texts ) assert example == [ '\u2581talk ing \u2581about \u2581ge ology' ]","title":"SentencePiecePrep"},{"location":"api/textprep.html#tokenwiser.textprep._sentpiece.SentencePiecePrep.download","text":"Download a pre-trained model from the bpemb project. You can see some examples of pre-trained models on the English sub-site. There are many languages available, but you should take care that you pick the right vocabulary size. Parameters: Name Type Description Default lang str language code required vocab_size int vocab size, can be 1000, 3000, 5000, 10000, 25000, 50000, 100000, 200000 required Source code in tokenwiser/textprep/_sentpiece.py @classmethod def download ( self , lang : str , vocab_size : int , filename : str = None ): \"\"\" Download a pre-trained model from the bpemb project. You can see some examples of pre-trained models on the [English](https://nlp.h-its.org/bpemb/en/) sub-site. There are many languages available, but you should take care that you pick the right vocabulary size. Arguments: lang: language code vocab_size: vocab size, can be 1000, 3000, 5000, 10000, 25000, 50000, 100000, 200000 \"\"\" url = f \"https://bpemb.h-its.org/ { lang } / { lang } .wiki.bpe.vs { vocab_size } .model\" if not filename : filename = f \" { lang } .wiki.bpe.vs { vocab_size } .model\" try : urllib . request . urlretrieve ( url = url , filename = filename ) except HTTPError : raise ValueError ( f \"Double check if the language ( { lang } ) and voacb size ( { vocab_size } ) combo exist.\" )","title":"download()"},{"location":"api/textprep.html#tokenwiser.textprep._phonetic.PhoneticTextPrep","text":"The ProneticPrep object prepares strings by encoding them phonetically. Parameters: Name Type Description Default kind type of encoding, either \"soundex\" , \" metaphone \" or \"nysiis\" required Usage: import spacy from tokenwiser.textprep import PhoneticTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = PhoneticTextPrep ( kind = \"soundex\" ) . transform ([ \"dinosaurus book\" ]) example2 = PhoneticTextPrep ( kind = \"metaphone\" ) . transform ([ \"dinosaurus book\" ]) example3 = PhoneticTextPrep ( kind = \"nysiis\" ) . transform ([ \"dinosaurus book\" ]) assert example1 [ 0 ] == 'D526 B200' assert example2 [ 0 ] == 'TNSRS BK' assert example3 [ 0 ] == 'DANASAR BAC'","title":"PhoneticTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._yake.YakeTextPrep","text":"Remove all text except meaningful key-phrases. Uses yake . Parameters: Name Type Description Default top_n number of key-phrases to select required unique only return unique keywords from the key-phrases required Usage: from tokenwiser.textprep import YakeTextPrep text = [ \"Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning\" ] example = YakeTextPrep ( top_n = 3 , unique = False ) . transform ( text ) assert example [ 0 ] == 'hosts data science acquiring kaggle google is acquiring'","title":"YakeTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._morph.SpacyMorphTextPrep","text":"Adds morphologic information to tokens in text. Usage: import spacy from tokenwiser.textprep import SpacyMorphTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyMorphTextPrep ( nlp ) . encode_single ( \"quick! duck!\" ) example2 = SpacyMorphTextPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"quick|Degree=Pos !|PunctType=Peri duck|Number=Sing !|PunctType=Peri\" assert example2 == \"hey| look|VerbForm=Inf a|Definite=Ind|PronType=Art duck|Number=Sing\"","title":"SpacyMorphTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._morph.SpacyPosTextPrep","text":"Adds part of speech information per token using spaCy. Parameters: Name Type Description Default model the spaCy model to use required lemma also lemmatize the text required fine_grained use fine grained parts of speech required Usage: import spacy from tokenwiser.textprep import SpacyPosTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyPosTextPrep ( nlp ) . encode_single ( \"we need to duck\" ) example2 = SpacyPosTextPrep ( nlp ) . encode_single ( \"hey look a duck\" ) assert example1 == \"we|PRON need|VERB to|PART duck|VERB\" assert example2 == \"hey|INTJ look|VERB a|DET duck|NOUN\"","title":"SpacyPosTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._morph.SpacyLemmaTextPrep","text":"Turns each token into a lemmatizer version using spaCy. Usage: import spacy from tokenwiser.textprep import SpacyLemmaTextPrep nlp = spacy . load ( \"en_core_web_sm\" ) example1 = SpacyLemmaTextPrep ( nlp ) . encode_single ( \"we are running\" ) example2 = SpacyLemmaTextPrep ( nlp ) . encode_single ( \"these are dogs\" ) assert example1 == 'we be run' assert example2 == 'these be dog'","title":"SpacyLemmaTextPrep"},{"location":"api/textprep.html#tokenwiser.textprep._snowball.SnowballTextPrep","text":"Applies the snowball stemmer to the text. There are 26 languages supported, for the full list check the list on the lefthand side on pypi . Usage: from tokenwiser.textprep import SnowballTextPrep single = SnowballTextPrep ( language = 'english' ) . encode_single ( \"Dogs like running\" ) assert single == \"Dog like run\" multi = Cleaner () . transform ([ \"Dogs like running\" , \"Cats like sleeping\" ]) assert multi == [ \"Dog like run\" , \"Cat like sleep\" ]","title":"SnowballTextPrep"},{"location":"api/wabbit.html","text":"wabbit \u00b6 from tokenwiser.wabbit import * In the wabbit submodule you can find a scikit-learn component based on vowpal wabbit . VowpalWabbitClassifier ( BaseEstimator , ClassifierMixin ) \u00b6 Vowpal Wabbit based text classifier. This object represents a simplified Vowpal Wabbit classifier that is compatible with scikit-learn. The only caveat is that the model expects text-arrays as opposed to numeric arrays. Parameters: Name Type Description Default n_loop the number of times the fit step should apply to the training data required n_gram number of n_grams to encode as well required learning_rate the learning rate to apply while training required Usage: from tokenwiser.wabbit import VowpalWabbitClassifier clf = VowpalWabbitClassifier () X = [ \"this is friendly\" , \"very friendly\" , \"i do not like you\" , \"the sky is blue\" ] y = [ \"pos\" , \"pos\" , \"neg\" , \"neutral\" ] # partial fitting for x_ , y_ in zip ( X , y ): clf . partial_fit ( x_ , y_ , classes = [ \"pos\" , \"neg\" , \"neutral\" ]) clf . predict ( X ) # batch fitting clf . fit ( X , y ) . predict ( X ) fit ( self , X , y ) \u00b6 Fit the model using X, y as training data. Parameters: Name Type Description Default X array-like, shape=(n_columns, n_samples, ) training data, must be text. required y labels required Source code in tokenwiser/wabbit/_vowpal.py def fit ( self , X , y ): \"\"\" Fit the model using X, y as training data. Arguments: X: array-like, shape=(n_columns, n_samples, ) training data, must be text. y: labels \"\"\" return self . partial_fit ( X , y , classes = list ( set ( y ))) partial_fit ( self , X , y , classes ) \u00b6 Incremental fit on a batch of samples. Parameters: Name Type Description Default X array-like, shape=(n_columns, n_samples, ) training data, must be text. required y labels required classes list of all the classes in the dataset required Source code in tokenwiser/wabbit/_vowpal.py def partial_fit ( self , X , y , classes ): \"\"\" Incremental fit on a batch of samples. Arguments: X: array-like, shape=(n_columns, n_samples, ) training data, must be text. y: labels classes: list of all the classes in the dataset \"\"\" if not isinstance ( X [ 0 ], str ): raise ValueError ( \"This model only accepts text as input.\" ) if not self . model : self . classes_ = classes self . idx_to_cls_ = { i + 1 : c for i , c in enumerate ( self . classes_ )} self . cls_to_idx_ = { c : i + 1 for i , c in enumerate ( self . classes_ )} self . model = pyvw . vw ( quiet = True , oaa = len ( classes ), ngram = self . n_gram , learning_rate = self . learning_rate , loss_function = \"logistic\" , probabilities = True , ) for loop in range ( self . n_loop ): for x_ , y_ in zip ( X , y ): try : self . model . learn ( f \" { self . cls_to_idx_ [ y_ ] } | { x_ } \" ) except RuntimeError as e : ex = f \" { self . cls_to_idx_ [ y_ ] } | { x_ } \" raise RuntimeError ( f \" { e } \\n culprit: { ex } \" ) return self predict ( self , X ) \u00b6 Perform classification on an array of test vectors X. Parameters: Name Type Description Default X array-like, shape=(n_columns, n_samples, ) training data, must be text. required Source code in tokenwiser/wabbit/_vowpal.py def predict ( self , X ): \"\"\" Perform classification on an array of test vectors X. Arguments: X: array-like, shape=(n_columns, n_samples, ) training data, must be text. \"\"\" argmax = self . predict_proba ( X ) . argmax ( axis = 1 ) return np . array ([ self . idx_to_cls_ [ a + 1 ] for a in argmax ]) predict_proba ( self , X ) \u00b6 Return probability estimates for the test vector X. Parameters: Name Type Description Default X array-like, shape=(n_columns, n_samples, ) training data, must be text. required Source code in tokenwiser/wabbit/_vowpal.py def predict_proba ( self , X ): \"\"\" Return probability estimates for the test vector X. Arguments: X: array-like, shape=(n_columns, n_samples, ) training data, must be text. \"\"\" check_is_fitted ( self , [ \"classes_\" , \"cls_to_idx_\" , \"idx_to_cls_\" ]) r = np . array ([ self . model . predict ( f \"| { x } \" ) for x in X ]) return r / r . sum ( axis = 1 ) . reshape ( - 1 , 1 )","title":"wabbit"},{"location":"api/wabbit.html#wabbit","text":"from tokenwiser.wabbit import * In the wabbit submodule you can find a scikit-learn component based on vowpal wabbit .","title":"wabbit"},{"location":"api/wabbit.html#tokenwiser.wabbit._vowpal.VowpalWabbitClassifier","text":"Vowpal Wabbit based text classifier. This object represents a simplified Vowpal Wabbit classifier that is compatible with scikit-learn. The only caveat is that the model expects text-arrays as opposed to numeric arrays. Parameters: Name Type Description Default n_loop the number of times the fit step should apply to the training data required n_gram number of n_grams to encode as well required learning_rate the learning rate to apply while training required Usage: from tokenwiser.wabbit import VowpalWabbitClassifier clf = VowpalWabbitClassifier () X = [ \"this is friendly\" , \"very friendly\" , \"i do not like you\" , \"the sky is blue\" ] y = [ \"pos\" , \"pos\" , \"neg\" , \"neutral\" ] # partial fitting for x_ , y_ in zip ( X , y ): clf . partial_fit ( x_ , y_ , classes = [ \"pos\" , \"neg\" , \"neutral\" ]) clf . predict ( X ) # batch fitting clf . fit ( X , y ) . predict ( X )","title":"VowpalWabbitClassifier"},{"location":"api/wabbit.html#tokenwiser.wabbit._vowpal.VowpalWabbitClassifier.fit","text":"Fit the model using X, y as training data. Parameters: Name Type Description Default X array-like, shape=(n_columns, n_samples, ) training data, must be text. required y labels required Source code in tokenwiser/wabbit/_vowpal.py def fit ( self , X , y ): \"\"\" Fit the model using X, y as training data. Arguments: X: array-like, shape=(n_columns, n_samples, ) training data, must be text. y: labels \"\"\" return self . partial_fit ( X , y , classes = list ( set ( y )))","title":"fit()"},{"location":"api/wabbit.html#tokenwiser.wabbit._vowpal.VowpalWabbitClassifier.partial_fit","text":"Incremental fit on a batch of samples. Parameters: Name Type Description Default X array-like, shape=(n_columns, n_samples, ) training data, must be text. required y labels required classes list of all the classes in the dataset required Source code in tokenwiser/wabbit/_vowpal.py def partial_fit ( self , X , y , classes ): \"\"\" Incremental fit on a batch of samples. Arguments: X: array-like, shape=(n_columns, n_samples, ) training data, must be text. y: labels classes: list of all the classes in the dataset \"\"\" if not isinstance ( X [ 0 ], str ): raise ValueError ( \"This model only accepts text as input.\" ) if not self . model : self . classes_ = classes self . idx_to_cls_ = { i + 1 : c for i , c in enumerate ( self . classes_ )} self . cls_to_idx_ = { c : i + 1 for i , c in enumerate ( self . classes_ )} self . model = pyvw . vw ( quiet = True , oaa = len ( classes ), ngram = self . n_gram , learning_rate = self . learning_rate , loss_function = \"logistic\" , probabilities = True , ) for loop in range ( self . n_loop ): for x_ , y_ in zip ( X , y ): try : self . model . learn ( f \" { self . cls_to_idx_ [ y_ ] } | { x_ } \" ) except RuntimeError as e : ex = f \" { self . cls_to_idx_ [ y_ ] } | { x_ } \" raise RuntimeError ( f \" { e } \\n culprit: { ex } \" ) return self","title":"partial_fit()"},{"location":"api/wabbit.html#tokenwiser.wabbit._vowpal.VowpalWabbitClassifier.predict","text":"Perform classification on an array of test vectors X. Parameters: Name Type Description Default X array-like, shape=(n_columns, n_samples, ) training data, must be text. required Source code in tokenwiser/wabbit/_vowpal.py def predict ( self , X ): \"\"\" Perform classification on an array of test vectors X. Arguments: X: array-like, shape=(n_columns, n_samples, ) training data, must be text. \"\"\" argmax = self . predict_proba ( X ) . argmax ( axis = 1 ) return np . array ([ self . idx_to_cls_ [ a + 1 ] for a in argmax ])","title":"predict()"},{"location":"api/wabbit.html#tokenwiser.wabbit._vowpal.VowpalWabbitClassifier.predict_proba","text":"Return probability estimates for the test vector X. Parameters: Name Type Description Default X array-like, shape=(n_columns, n_samples, ) training data, must be text. required Source code in tokenwiser/wabbit/_vowpal.py def predict_proba ( self , X ): \"\"\" Return probability estimates for the test vector X. Arguments: X: array-like, shape=(n_columns, n_samples, ) training data, must be text. \"\"\" check_is_fitted ( self , [ \"classes_\" , \"cls_to_idx_\" , \"idx_to_cls_\" ]) r = np . array ([ self . model . predict ( f \"| { x } \" ) for x in X ]) return r / r . sum ( axis = 1 ) . reshape ( - 1 , 1 )","title":"predict_proba()"},{"location":"guide/sklearn.html","text":"Scikit-Learn pipelines are amazing but they are not perfect for simple text use-cases. The standard pipeline does not allow for interactive learning. You can apply .fit but that's it. Even if the tools inside of the pipeline have a .partial_fit available, the pipeline doesn't allow it. The CountVectorizer is great, but we might need some more text-tricks at our disposal that are specialized towards text to make this object more effective. Part of what this library does is give more tools that extend scikit-learn for simple text classification problems. In this document we will showcase some of the main features. Text Preparation Tools \u00b6 Let's first discuss a basic pipeline for text inside of scikit-learn. Base Pipeline \u00b6 This simplest text classification pipeline in scikit-learn looks like this; from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import SGDClassifier pipe = make_pipeline ( CountVectorizer (), SGDClassifier () ) This pipeline will encode words as sparse features before passing them on to the logistic regression model. This pattern is very common and has proven to work well enough for many English text classification tasks. The nice thing about using a SGDClassifier is that we're able to learn from our data even if the dataset does not fit in memory. We can call .partial_fit instead of .fit and learn in a more \"online\" setting. That said, there are things we can do even to this pipeline to make it better. Spelling Errors \u00b6 When you are classifying online texts you are often confronted with spelling errors. To deal with this you'd typically use a CountVectorizer with a character-level analyzer such that you also encode subwords. With all of these subwords around, we'll be more robust against spelling errors. The downside of this approach is that you might wonder if we really need all these subwords. So how about this, let's add a step that will turn our text into subwords by splitting up hyphens. from tokenwiser.textprep import HyphenTextPrep multi = HyphenTextPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ] The HyphenTextPrep preprocessor is a TextPrep -object. For all intents and purposes these are scikit-learn compatible preprocessing components but they all output strings instead of arrays. What's nice about these though is that you can \"retokenize\" the original text. This allows you to use the subtokens as if they were tokens which might help keep your pipelines lightweight while still keeping them robust against certain spelling errors. Long Texts \u00b6 There are some other tricks that you might want to apply for longer texts. Maybe you want to summarise a text before vectorizing it. So maybe it'd be nice to use a transformer that keeps only the most important tokens. A neat heuristic toolkit for this is yake (you can find a demo here ). This package also features a scikit-learn compatible component for it. from tokenwiser.textprep import YakeTextPrep text = [ \"Sources tell us that Google is acquiring Kaggle, \\ a platform that hosts data science and machine learning\" ] example = YakeTextPrep ( top_n = 3 , unique = False ) . transform ( text ) assert example [ 0 ] == 'hosts data science acquiring kaggle google is acquiring' The idea here is to reduce the text down to only the most important words. Again, this trick might keep the algorithm lightweight and this trick will go a lot further than most \"stopword\"-lists. Bag of Tricks! \u00b6 The goal of this library is to host a few meaningful tricks that might be helpful. Here's some more; Cleaner lowercase text remove all non alphanumeric characters. Identity just keeps the text as is, useful when constructing elaborate pipelines. PhoneticTextPrep translate text into a phonetic encoding. SpacyPosTextPrep add part of speech infomation to the text using spaCy. SpacyLemmaTextPrep lemmatize the text using spaCy. All of these tools are part of the textprep submodule and are documented in detail here . Pipeline Tools \u00b6 Pipeline components are certainly nice. But maybe we can go a step further for text. Maybe we can make better pipelines for text too! Concatenate Text \u00b6 In scikit-learn you would use FeatureUnion or make_union to concatenate features in a pipeline. Ut is assumed that transformers output arrays that need to be concatenated so the result of a concatenation is always a 2D array. This can be a bit awkward if you're using text preprocessors. The reason why we want to keep everything a string is so that the CountVectorizer from scikit-learn can properly encode it. That is why this library comes with a special union component: TextConcat . It concatenates the output of text-prep tools into a string instead of an array. Note that we also pack a convenient make_concat function too. from sklearn.pipeline import make_pipeline from tokenwiser.pipeline import make_concat from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), ) output = pipe . fit_transform ([ \"hello astrology!!!!\" ]) assert output == [ 'hello astrology hel lo astro logy' ] Again, we see that we're taking a text input and that we're generating a text output. The make_concat is making sure that we concatenate strings, not arrays! This is great when we want to follow up with a `CountVectorizer! from sklearn.pipeline import make_pipeline from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import CountVectorizer from tokenwiser.pipeline import make_concat from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), CountVectorizer (), LogisticRegression () ) The mental picture for pipe -pipeline looks like the diagram below. Partial Fit \u00b6 We can go a step further though. The scikit-learn pipeline follows the fit/predict API. That means that we cannot use .partial_fit () . Even if all the components in the pipeline are compatible with the partial_fit/predict API. That is why this library also introduced components for mini-batch learning: PartialPipeline and make_partial_pipeline In these scenarios you will need to swap out the CountVectorizer with a HashVectorizer in order to be able to learn from new data comming in. from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep from tokenwiser.pipeline import make_concat , make_partial_pipeline pipe = make_partial_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), HashingVectorizer (), SGDClassifier () ) This pipe -Pipeline is scikit-learn compatible for all intents and purposes but it has the option of learning from batches of data via partal_fit . This is great because it means that you're able to classify text even when it doesn't fit into memory! Note that all of the TextPrep -components in this library allow for partial_fit . To make a partial_fit actually work you will need to supply the names of the classes at learning time. Otherwise you might accidentally get a batch that only contains one class and the algorithm would become numerically unstable. import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep from tokenwiser.pipeline import make_partial_pipeline , make_partial_union pipe = make_partial_pipeline ( Cleaner (), make_partial_union ( make_partial_pipeline ( Identity (), HashingVectorizer ()), make_partial_pipeline ( HyphenTextPrep (), HashingVectorizer ()) ), SGDClassifier () ) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) for loop in range ( 3 ): # It might make sense to loop over the same dataset multiple times # if the dataset is small. For larger datasets this isn't recommended. pipe . partial_fit ( X , y , classes = [ 0 , 1 ]) assert np . all ( pipe . predict ( X ) == np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ])) Concatenate Features \u00b6 The standard FeatureUnion from scikit-learn also does not allow for .partial_fit . So we've added a PartialFeatureUnion class and a make_partial_union function to this library as well. import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep from tokenwiser.pipeline import make_partial_pipeline , make_partial_union pipe = make_partial_pipeline ( Cleaner (), make_partial_union ( make_partial_pipeline ( Identity (), HashingVectorizer ()), make_partial_pipeline ( HyphenTextPrep (), HashingVectorizer ()) ), SGDClassifier () ) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) for loop in range ( 3 ): pipe . partial_fit ( X , y , classes = [ 0 , 1 ]) assert np . all ( pipe . predict ( X ) == np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]))","title":"Scikit-Learn"},{"location":"guide/sklearn.html#text-preparation-tools","text":"Let's first discuss a basic pipeline for text inside of scikit-learn.","title":"Text Preparation Tools"},{"location":"guide/sklearn.html#base-pipeline","text":"This simplest text classification pipeline in scikit-learn looks like this; from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.linear_model import SGDClassifier pipe = make_pipeline ( CountVectorizer (), SGDClassifier () ) This pipeline will encode words as sparse features before passing them on to the logistic regression model. This pattern is very common and has proven to work well enough for many English text classification tasks. The nice thing about using a SGDClassifier is that we're able to learn from our data even if the dataset does not fit in memory. We can call .partial_fit instead of .fit and learn in a more \"online\" setting. That said, there are things we can do even to this pipeline to make it better.","title":"Base Pipeline"},{"location":"guide/sklearn.html#spelling-errors","text":"When you are classifying online texts you are often confronted with spelling errors. To deal with this you'd typically use a CountVectorizer with a character-level analyzer such that you also encode subwords. With all of these subwords around, we'll be more robust against spelling errors. The downside of this approach is that you might wonder if we really need all these subwords. So how about this, let's add a step that will turn our text into subwords by splitting up hyphens. from tokenwiser.textprep import HyphenTextPrep multi = HyphenTextPrep () . transform ([ \"geology\" , \"astrology\" ]) assert multi == [ 'geo logy' , 'as tro logy' ] The HyphenTextPrep preprocessor is a TextPrep -object. For all intents and purposes these are scikit-learn compatible preprocessing components but they all output strings instead of arrays. What's nice about these though is that you can \"retokenize\" the original text. This allows you to use the subtokens as if they were tokens which might help keep your pipelines lightweight while still keeping them robust against certain spelling errors.","title":"Spelling Errors"},{"location":"guide/sklearn.html#long-texts","text":"There are some other tricks that you might want to apply for longer texts. Maybe you want to summarise a text before vectorizing it. So maybe it'd be nice to use a transformer that keeps only the most important tokens. A neat heuristic toolkit for this is yake (you can find a demo here ). This package also features a scikit-learn compatible component for it. from tokenwiser.textprep import YakeTextPrep text = [ \"Sources tell us that Google is acquiring Kaggle, \\ a platform that hosts data science and machine learning\" ] example = YakeTextPrep ( top_n = 3 , unique = False ) . transform ( text ) assert example [ 0 ] == 'hosts data science acquiring kaggle google is acquiring' The idea here is to reduce the text down to only the most important words. Again, this trick might keep the algorithm lightweight and this trick will go a lot further than most \"stopword\"-lists.","title":"Long Texts"},{"location":"guide/sklearn.html#bag-of-tricks","text":"The goal of this library is to host a few meaningful tricks that might be helpful. Here's some more; Cleaner lowercase text remove all non alphanumeric characters. Identity just keeps the text as is, useful when constructing elaborate pipelines. PhoneticTextPrep translate text into a phonetic encoding. SpacyPosTextPrep add part of speech infomation to the text using spaCy. SpacyLemmaTextPrep lemmatize the text using spaCy. All of these tools are part of the textprep submodule and are documented in detail here .","title":"Bag of Tricks!"},{"location":"guide/sklearn.html#pipeline-tools","text":"Pipeline components are certainly nice. But maybe we can go a step further for text. Maybe we can make better pipelines for text too!","title":"Pipeline Tools"},{"location":"guide/sklearn.html#concatenate-text","text":"In scikit-learn you would use FeatureUnion or make_union to concatenate features in a pipeline. Ut is assumed that transformers output arrays that need to be concatenated so the result of a concatenation is always a 2D array. This can be a bit awkward if you're using text preprocessors. The reason why we want to keep everything a string is so that the CountVectorizer from scikit-learn can properly encode it. That is why this library comes with a special union component: TextConcat . It concatenates the output of text-prep tools into a string instead of an array. Note that we also pack a convenient make_concat function too. from sklearn.pipeline import make_pipeline from tokenwiser.pipeline import make_concat from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), ) output = pipe . fit_transform ([ \"hello astrology!!!!\" ]) assert output == [ 'hello astrology hel lo astro logy' ] Again, we see that we're taking a text input and that we're generating a text output. The make_concat is making sure that we concatenate strings, not arrays! This is great when we want to follow up with a `CountVectorizer! from sklearn.pipeline import make_pipeline from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import CountVectorizer from tokenwiser.pipeline import make_concat from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep pipe = make_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), CountVectorizer (), LogisticRegression () ) The mental picture for pipe -pipeline looks like the diagram below.","title":"Concatenate Text"},{"location":"guide/sklearn.html#partial-fit","text":"We can go a step further though. The scikit-learn pipeline follows the fit/predict API. That means that we cannot use .partial_fit () . Even if all the components in the pipeline are compatible with the partial_fit/predict API. That is why this library also introduced components for mini-batch learning: PartialPipeline and make_partial_pipeline In these scenarios you will need to swap out the CountVectorizer with a HashVectorizer in order to be able to learn from new data comming in. from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep from tokenwiser.pipeline import make_concat , make_partial_pipeline pipe = make_partial_pipeline ( Cleaner (), make_concat ( Identity (), HyphenTextPrep ()), HashingVectorizer (), SGDClassifier () ) This pipe -Pipeline is scikit-learn compatible for all intents and purposes but it has the option of learning from batches of data via partal_fit . This is great because it means that you're able to classify text even when it doesn't fit into memory! Note that all of the TextPrep -components in this library allow for partial_fit . To make a partial_fit actually work you will need to supply the names of the classes at learning time. Otherwise you might accidentally get a batch that only contains one class and the algorithm would become numerically unstable. import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep from tokenwiser.pipeline import make_partial_pipeline , make_partial_union pipe = make_partial_pipeline ( Cleaner (), make_partial_union ( make_partial_pipeline ( Identity (), HashingVectorizer ()), make_partial_pipeline ( HyphenTextPrep (), HashingVectorizer ()) ), SGDClassifier () ) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) for loop in range ( 3 ): # It might make sense to loop over the same dataset multiple times # if the dataset is small. For larger datasets this isn't recommended. pipe . partial_fit ( X , y , classes = [ 0 , 1 ]) assert np . all ( pipe . predict ( X ) == np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]))","title":"Partial Fit"},{"location":"guide/sklearn.html#concatenate-features","text":"The standard FeatureUnion from scikit-learn also does not allow for .partial_fit . So we've added a PartialFeatureUnion class and a make_partial_union function to this library as well. import numpy as np from sklearn.linear_model import SGDClassifier from sklearn.feature_extraction.text import HashingVectorizer from tokenwiser.textprep import Cleaner , Identity , HyphenTextPrep from tokenwiser.pipeline import make_partial_pipeline , make_partial_union pipe = make_partial_pipeline ( Cleaner (), make_partial_union ( make_partial_pipeline ( Identity (), HashingVectorizer ()), make_partial_pipeline ( HyphenTextPrep (), HashingVectorizer ()) ), SGDClassifier () ) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) for loop in range ( 3 ): pipe . partial_fit ( X , y , classes = [ 0 , 1 ]) assert np . all ( pipe . predict ( X ) == np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]))","title":"Concatenate Features"},{"location":"guide/spacy.html","text":"This is where we'll elaborate on the spaCy tools. Under construction.","title":"spaCy"}]}